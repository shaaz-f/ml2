{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA1wjGN1rDtK"
      },
      "source": [
        "# **Assignment 3**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G4oBA6DrIyr"
      },
      "source": [
        "### Insert your details below. You should see a green checkmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWWJ4OMvp-2i"
      },
      "outputs": [],
      "source": [
        "# Input student details (replace these with actual input values)\n",
        "student = {\n",
        "    \"name\": \"\",                    # Replace with your name\n",
        "    \"email\": \"@ualberta.ca\",       # Replace with your email\n",
        "    \"ccid\": \"\",                    # Replace with your CCID\n",
        "    \"idnumber\": 1,                 # Replace with your ID number\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kvuOlM0p_vZ"
      },
      "outputs": [],
      "source": [
        "# Define the default and user-provided student dictionaries\n",
        "def_student = {\n",
        "    \"name\": \"NAME as in eclass\",\n",
        "    \"email\": \"UofA Email\",\n",
        "    \"ccid\": \"CCID\",\n",
        "    \"idnumber\": 0\n",
        "}\n",
        "\n",
        "\n",
        "# Validation checks\n",
        "assert set(def_student.keys()) == set(student.keys()),   \"You don't have all the right entries! Make sure you have `name`, `email`, `ccid`, `idnumber`. ❌\"\n",
        "assert not any(value == \"\" for value in student.values()), \"You haven't filled in all your details! No field should be empty. ❌\"\n",
        "assert all(isinstance(student[k], type(def_student[k])) for k in def_student),    \"Your types seem to be off: `name::String`, `email::String`, `ccid::String`, `idnumber::Int`. ❌\"\n",
        "assert student[\"email\"].endswith(\"@ualberta.ca\"), \"Your email must end with '@ualberta.ca'. ❌\"\n",
        "\n",
        "print(f\"Welcome {student['name']}! ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VisLDpX6rAkr"
      },
      "source": [
        "# Preamble\n",
        "\n",
        "- Loading Packages\n",
        "- Generating Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsrPQX1tZF9U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd\n",
        "from itertools import product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_yP41IrYmLR"
      },
      "source": [
        "\n",
        "# Toy Data\n",
        "\n",
        "Let's first start by importing a dataset for use throughout the assignment.\n",
        "Later in the assignment, we will be using the full dataset for evaluating our models.\n",
        "But for debugging purposes, we will start with a small subset of the data to test our models very quickly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlnWQma9YfGh"
      },
      "outputs": [],
      "source": [
        "def isclose(a, b, prec=1e-8):\n",
        "    return np.linalg.norm(a - b) < prec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_XxBa6KYvYi"
      },
      "outputs": [],
      "source": [
        "def one_hot(x):\n",
        "    # Collect unique values in order of appearance\n",
        "    unique_vals = []\n",
        "    for v in x:\n",
        "        if v not in unique_vals:\n",
        "            unique_vals.append(v)\n",
        "    unique_vals = np.array(unique_vals)\n",
        "\n",
        "    # Create a Boolean matrix of shape (num_unique, len(x))\n",
        "    onehot_matrix = (unique_vals[:, None] == x[None, :]).astype(np.float32)\n",
        "\n",
        "    # Transpose so that each row corresponds to one sample:\n",
        "    # Final shape becomes (len(x), num_unique)\n",
        "    return onehot_matrix.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3Sksx6ICGpE"
      },
      "outputs": [],
      "source": [
        "def prepMNIST(samples: int):\n",
        "\n",
        "    # Load the MNIST dataset.\n",
        "    (x_train, y_train), _ = mnist.load_data()\n",
        "\n",
        "    # print(\"Shape of x_train:\", x_train.shape)\n",
        "    # print(\"Shape of y_train:\", y_train.shape)\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
        "\n",
        "    # One-hot encode the labels.\n",
        "    y_train = one_hot(y_train)\n",
        "\n",
        "    return x_train[:samples], y_train[:samples]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwSSd6qzCepk"
      },
      "outputs": [],
      "source": [
        "toy_x, toy_y = prepMNIST(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw9v-v_9DjPr"
      },
      "outputs": [],
      "source": [
        "print(toy_x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFSK3TN3DanL"
      },
      "outputs": [],
      "source": [
        "print(toy_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXWJVqKRcpR1"
      },
      "source": [
        "# Supervised Autoencoders\n",
        "\n",
        "In this part of the notebook, you will implement a supervised autoencoder with a single hidden layer.\n",
        "You will use backpropagation in order to take gradients w.r.t. the weights for a joint loss function:\n",
        "$$\n",
        "\\ell(\\mathbf{x}, \\mathbf{y}; \\mathbf{W}^{(2)}, \\mathbf{W}_x^{(1)}, \\mathbf{W}_y^{(1)}) = \\ell_y(\\hat{\\mathbf{y}}, \\mathbf{y}) + \\beta \\ell_x(\\hat{\\mathbf{x}}, \\mathbf{x})\n",
        "$$\n",
        "\n",
        "where $\\beta \\ge 0$ is a hyperparameter that determines with how much weight the autoencoder portion of the SAE influences the hidden dimension.\n",
        "\n",
        "As outlined in the assignment pdf, we will use identity activations (no activation, resulting in a linear function) for every layer except the heads corresponding to $\\hat{\\mathbf{y}}$. For this supervised part, because we are doing multinomial logistic regression, we use a softmax activation.\n",
        "The loss $\\ell_y$ is the multinomial cross-entropy for the \"$\\hat{\\mathbf{y}}$\" head of the neural network and the loss $\\ell_x$ is the mean squared error for the \"$\\hat{\\mathbf{x}}$\" head, where $\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{x} \\mathbf{W}^{(2)} \\mathbf{W}_y^{(1)})$ and $\\hat{\\mathbf{x}} = \\mathbf{x} \\mathbf{W}^{(2)}  \\mathbf{W}_x^{(1)}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZyOszYjcNAB"
      },
      "outputs": [],
      "source": [
        "class SAE:\n",
        "    def __init__(self, features, hidden, outputs, rng):\n",
        "        self.W2 = rng.normal(0, 1 / (features + hidden), (features, hidden))\n",
        "        self.Wx = rng.normal(0, 1 / (features + hidden), (hidden, features))\n",
        "        self.Wy = rng.normal(0, 1 / (hidden + outputs), (hidden, outputs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgP8RWA4dj3e"
      },
      "outputs": [],
      "source": [
        "class SAEParams:\n",
        "    def __init__(self, β=None, alpha=None, epochs=None, hidden=None, vals=None):\n",
        "        if vals is not None:\n",
        "            self.β, self.alpha, self.epochs, self.hidden = vals\n",
        "        else:\n",
        "            self.β = β\n",
        "            self.alpha = alpha\n",
        "            self.epochs = epochs\n",
        "            self.hidden = hidden\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7sQWzIreIDh"
      },
      "source": [
        "## Utility functions\n",
        "Here, we implement a few utility functions which we will need throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHnyVk9zdpdO"
      },
      "outputs": [],
      "source": [
        "def softmax(X):\n",
        "    exp_X = np.exp(X)\n",
        "    return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaxEcEDwCVtv"
      },
      "outputs": [],
      "source": [
        "def forward(sae, X):\n",
        "    h = np.dot(X, sae.W2)\n",
        "    X_hat = np.dot(h, sae.Wx)\n",
        "    z = np.dot(h, sae.Wy)\n",
        "    Y_hat = softmax(z)\n",
        "    return X_hat, Y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWmxaEZke_XQ"
      },
      "outputs": [],
      "source": [
        "def oneHotMax(X):\n",
        "    def _inner(a):\n",
        "        idx = np.argmax(a)\n",
        "        out = np.zeros_like(a, dtype=int)\n",
        "        out[idx] = 1\n",
        "        return out\n",
        "\n",
        "    # Apply _inner function to each row of the matrix\n",
        "    return np.vstack([_inner(row) for row in X])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGD3AkEgfNuS"
      },
      "source": [
        "## Gradient Computation\n",
        "In this section, we will implement the gradient of the SAE loss function. The loss function is defined at the beginning of the _Supervised Autoencoders_ section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IXY-TZ5fHab"
      },
      "outputs": [],
      "source": [
        "def gradient(sae, params, X, Y):\n",
        "    # Compute the gradient of the SAE parameters given inputs (X, Y)\n",
        "\n",
        "    h = np.dot(X, sae.W2)\n",
        "\n",
        "    # samples x features matrix\n",
        "    X_hat = np.dot(h, sae.Wx)\n",
        "\n",
        "    # samples x outputs matrix\n",
        "    z = np.dot(h, sae.Wy)\n",
        "    Y_hat = softmax(z)\n",
        "\n",
        "    # Gradient for the Wx head\n",
        "    delta_wx = params.β * (X_hat - X)  # (samples, features)\n",
        "    grad_Wx = np.dot(h.T, delta_wx)  # (hidden, features)\n",
        "\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # Gradient for the Wy head\n",
        "\n",
        "\n",
        "\t  ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # Gradient for W2\n",
        "\n",
        "\t  ### END SOLUTION\n",
        "\n",
        "    return grad_Wx, grad_Wy, grad_W2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3bFFxbzftbr"
      },
      "outputs": [],
      "source": [
        "# ##############\n",
        "# Test Block\n",
        "# ##############\n",
        "\n",
        "\n",
        "# Let's first double check that you are getting the right shapes\n",
        "sae = SAE(28*28, 128, 10, np.random.RandomState(1))  # Using RandomState as RNG\n",
        "params = SAEParams(0.5, 0.001, 30, 128)\n",
        "\n",
        "# Assuming the toy_x and toy_y are defined somewhere (as input data)\n",
        "delWx, delWy, delW2 = gradient(sae, params, toy_x, toy_y)\n",
        "\n",
        "# Check if the shapes of the gradients are correct\n",
        "assert (delWx.shape == (128, 28*28)) and (delWy.shape == (128, 10)) and (delW2.shape == (28*28, 128)), \"Incorrect shape of the gradient. ❌\"\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(367)\n",
        "\n",
        "sae = SAE(3, 4, 2, np.random.RandomState(2))\n",
        "params = SAEParams(2.0, 0.01, 30, 4)\n",
        "\n",
        "x = np.ones((5, 3))\n",
        "y = rng.binomial(1, 0.5, (5, 2))  # Random binary values for y (Bernoulli distribution)\n",
        "\n",
        "delWx, delWy, delW2 = gradient(sae, params, x, y)\n",
        "\n",
        "# Expected values for the gradients\n",
        "expect_delWx = np.array([\n",
        "    [ 4.54203257,  4.25261002,  5.2015941 ],\n",
        "    [ 2.51137886,  2.35135146,  2.87606336],\n",
        "    [ 1.50355791,  1.40774979,  1.72189385],\n",
        "    [-3.73462787, -3.4966539,  -4.27694386]\n",
        "    ])\n",
        "expect_delWy = np.array([\n",
        "        [-0.61700488, -0.31675102],\n",
        "        [-0.3411541 , -0.17513785],\n",
        "        [-0.20424833, -0.10485471],\n",
        "        [ 0.50732433,  0.26044445]\n",
        "])\n",
        "expect_delW2 = np.array([\n",
        "[ 0.43764503, -1.2914451,   2.0948465,   1.65034573],\n",
        "[ 0.43764503, -1.2914451,   2.0948465,   1.65034573],\n",
        "[ 0.43764503, -1.2914451,   2.0948465,   1.65034573]\n",
        "])\n",
        "\n",
        "# Compare the computed gradients with expected gradients\n",
        "assert np.allclose(delWx, expect_delWx), \"Incorrect values for delWx. ❌\"\n",
        "assert np.allclose(delWy, expect_delWy), \"Incorrect values for delWy. ❌\"\n",
        "assert np.allclose(delW2, expect_delW2), \"Incorrect values for delW2. ❌\"\n",
        "\n",
        "\n",
        "print(\"Gradient check passed successfully!✅\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHBMZRwDAyqH"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Here you wil implement the training loop for the SAE, where you do SGD for multiple epochs with a constant stepsize. Remember to randomly shuffle the data for each epoch. Here we simply use vanilla SGD, with 1 sample per update (a mini-batch of size 1) and use a constant stepsize (rather than RMSProp and Adam). The reason we do this is to let you focus on the gradients for the SAE, rather than the additional details of the optimizer. Further, for this model and dataset, this provides sufficiently good performance. Of course, you can always test for yourself the impact of using mini-batches and improved stepsizes. But for your submission please use vanilla SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oRVoDQPF7FO"
      },
      "outputs": [],
      "source": [
        "def train(X, Y, params, seed):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    samples, features = X.shape\n",
        "    classes = Y.shape[1]\n",
        "\n",
        "    # Build the SAE model with N hidden units\n",
        "    sae = SAE(features, params.hidden, classes, rng)\n",
        "\n",
        "    # Train the model using SGD for params.epochs number of epochs\n",
        "    # Use only a single sample for each iteration (stochastic gradient descent)\n",
        "    # Remember to randomly shuffle the samples for each epoch\n",
        "    for _ in range(params.epochs):\n",
        "        ### BEGIN SOLUTION\n",
        "        # Randomly shuffle the samples using rng permution\n",
        "        sample_indices =\n",
        "\n",
        "        for sample in sample_indices:\n",
        "\n",
        "\n",
        "            # Get the single sample\n",
        "\n",
        "            # Compute the gradients\n",
        "\n",
        "            # Update the weights using gradient descent\n",
        "\n",
        "        ### END SOLUTION\n",
        "\n",
        "    return sae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePvh7X9GIDZ5"
      },
      "outputs": [],
      "source": [
        "# ##############\n",
        "# Test Block\n",
        "# ##############\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "rng = np.random.RandomState(1234)\n",
        "\n",
        "# Create synthetic data\n",
        "n = 20  # Number of samples\n",
        "d = 5   # Number of features\n",
        "m = 2   # Number of classes\n",
        "\n",
        "# Generate two groups of data with different means\n",
        "X1 = rng.normal(0.0, 1.0, (n // 2, d))\n",
        "X2 = rng.normal(5.0, 1.0, (n // 2, d))\n",
        "\n",
        "# Combine the two groups and shuffle the data\n",
        "X = np.vstack((X1, X2))\n",
        "perm = rng.permutation(n)\n",
        "X = X[perm, :]\n",
        "\n",
        "# Normalize the features\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Generate labels (1 for X1, 2 for X2) and one-hot encode them\n",
        "Y = np.hstack((np.ones(n // 2), 2 * np.ones(n // 2)))\n",
        "Y = np.eye(m)[Y.astype(int) - 1]  # One-hot encode the labels\n",
        "\n",
        "# Set hyperparameters\n",
        "params = SAEParams(0.1, 0.1, 10, 10)\n",
        "\n",
        "# Train the model\n",
        "sae = train(X, Y, params, 1234)\n",
        "\n",
        "# Check the learned weights against expected values\n",
        "\n",
        "t1_c = sae.Wx[:2, :]\n",
        "# Checking the first 2 rows of sae.Wx (hidden units x features)\n",
        "t1 = np.allclose(t1_c,\n",
        "  [[-0.0621649, -0.19708555, -0.24813198, -0.14997647, -0.1899449],\n",
        "  [ 0.01971644,  0.12749735,  0.04233966,  0.11630627,  0.15949385]],\n",
        "                  atol=1e-6)\n",
        "\n",
        "t2_c =  sae.Wy[:5, :]\n",
        "# Checking the first 5 rows of sae.Wy (hidden units x output classes)\n",
        "t2 = np.allclose(t2_c,\n",
        "  [[ 0.17618548, -0.10470722],\n",
        "  [ 0.14259881, -0.07685819],\n",
        "  [ 0.15505535, -0.00107455],\n",
        "  [ 0.01042294, -0.11451652],\n",
        "  [ 0.11057633, -0.11528749]],\n",
        "                  atol=1e-6)\n",
        "\n",
        "t3_c= sae.W2[1:3, 4:8]\n",
        "\n",
        "# Checking the first 2 rows of sae.W2 (features x hidden units)\n",
        "t3 = np.allclose(t3_c,\n",
        "  [[ 0.23729971,  0.40224271, -0.147294,    0.09083973],\n",
        "  [ 0.00425958, -0.39550697,  0.16109506, -0.09772874]],\n",
        "                  atol=1e-6)\n",
        "\n",
        "\n",
        "# Check if all tests pass\n",
        "assert t1, \"Incorrect values for Wx. ❌\"\n",
        "assert t2, \"Incorrect values for Wy. ❌\"\n",
        "assert t3, \"Incorrect values for W2. ❌\"\n",
        "\n",
        "print(\"Training test passed successfully!✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygPHEt9JYHMy"
      },
      "source": [
        "# Evaluating our model\n",
        "\n",
        "Before we can do anything else (tuning hyperparameters, reporting statistics about our model, etc.), we must first decide how our model will be evaluated. In class, we primarily focused on classification accuracy. But other metrics are often used for classification, that better reflect how well the model performed for different classes and if it is skewed towards predicting one class more than others. These metrics can provide a more nuanced picture of the model.\n",
        "\n",
        "To start, we will treat each class separately: for example, what is the true positive rate for the digit 2?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zytUeMS8YMeJ"
      },
      "source": [
        "\n",
        "## True/False Positives/Negatives\n",
        "\n",
        "To begin, let's implement fuctions which count the true/false positives and negatives. A _true positive_ is a prediction that was predicted positive when it has a postive label (which we represent as a 1). A _false positive_ is a prediction that was predicted positive when is has a negative label. A _true negative_ is a prediction that was predicted negative when it has a negative label (which we represent as a 0). A _false negative_ is a prediction that was predicted negative when is has a positive label. We provide the code for true positives. It is your job to complete the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWZYRu-7IPfR"
      },
      "outputs": [],
      "source": [
        "# True positive\n",
        "def tp(y, y_hat):\n",
        "    return np.sum(y_hat[y == 1])\n",
        "\n",
        "# True negative\n",
        "def tn(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# False positive\n",
        "def fp(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# False negative\n",
        "def fn(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV8ewUdZYq-v"
      },
      "outputs": [],
      "source": [
        "# ##############\n",
        "# Test Block\n",
        "# ##############\n",
        "\n",
        "__y = np.array([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
        "__y_hat = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "assert tp(__y, __y_hat) == 3, \"Wrong tp number: ❌\"\n",
        "assert tn(__y, __y_hat) == 4, \"Wrong tn number: ❌\"\n",
        "assert fp(__y, __y_hat) == 6, \"Wrong fp number: ❌\"\n",
        "assert fn(__y, __y_hat) == 8, \"Wrong fn number: ❌\"\n",
        "\n",
        "print(\"Tests passed successfully!✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3xmNtF_ZjB_"
      },
      "source": [
        "\n",
        "Great! You've implemented four functions to count the number of:\n",
        "\n",
        "* True positives\n",
        "* True negatives\n",
        "* False positives\n",
        "* False negatives\n",
        "\n",
        "## Evaluation criteria\n",
        "\n",
        "Often, we'd like to consider the rates of true positives/negatives and false positives/negatives. Additionally, we often want to consider the accuracy of our predictions as well as the _precision_ and _recall_, which are defined as:\n",
        "\n",
        "$$\n",
        "\\text{Precision: } \\quad pr(y, \\hat y) = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Recall: } \\quad rc(y, \\hat y) = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "These definitions extend even to the multiclass setting. We can compute the precision and recall for each class. With precision, we get a sense of: when we classified an item as class j, was it actually class j? Precision is high when we are careful about assigning class j to an item. For example, imagine there are 100 samples that are class j. Even if we only label 10 of these as class j and do not incorrectly label any other items as class j, then our Precision = 1 which is maximal (here TP = 10 and FP = 0). Mentally, we can replace the term TP with Correctly-Labeled-as-Class-j and FP with Incorrectly-Labeled-as-Class-j.\n",
        "\n",
        "Recall asks: did we find all the items that were labeled as class j? Recall can be high when precision is low. For example, if we label all items in our dataset as class j, then we would have Recall = 1 (False Negatives = 0). We can think of FN as Incorrectly-Not-Labeled-as-Class-j or Should-Have-Been-Labeled-as-Class-j. If we label everything as class j, then there are no items that Should-Have-Been-Labeled-as-Class-j, since it was already labeled class j. Of course, this labelling will have bad Precision. Ideally, we want to get high Recall and high Precision.\n",
        "\n",
        "Reporting more metrics gives a multi-faceted view on your learned predictor. We will report all of them here, so you can see what they would look like. We have implemented some of the below functions for you: True Positive Rate, Accuracy and Error. It is your job to fill in the rest. Again, you will see checkmarks beside the metrics once they are implemented correctly.\n",
        "\n",
        "* True positive rate\n",
        "* False positive rate\n",
        "* True negative rate\n",
        "* False negative rate\n",
        "* Precision\n",
        "* Recall\n",
        "* Accuracy\n",
        "* Error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjxInFV8Y86N"
      },
      "outputs": [],
      "source": [
        "# True positive rate (Recall)\n",
        "def tpr(y, y_hat):\n",
        "    return tp(y, y_hat) / (tp(y, y_hat) + fn(y, y_hat))\n",
        "\n",
        "# False positive rate\n",
        "def fpr(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# True negative rate\n",
        "def tnr(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# False negative rate\n",
        "def fnr(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# Precision\n",
        "def pr(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# Recall\n",
        "def rc(y, y_hat):\n",
        "    ### BEGIN SOLUTION\n",
        "    return\n",
        "    ### END SOLUTION\n",
        "\n",
        "# Accuracy\n",
        "def accuracy(y, y_hat):\n",
        "    return np.sum(y == y_hat) / len(y)\n",
        "\n",
        "# Error\n",
        "def error(y, y_hat):\n",
        "    return 1 - accuracy(y, y_hat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXMwLfFaNbT"
      },
      "outputs": [],
      "source": [
        "# ##############\n",
        "# Test Block\n",
        "# ##############\n",
        "\n",
        "assert np.isclose(tpr(__y, __y_hat), 0.2727272727272727),\"True positive rate: ❌\"\n",
        "assert np.isclose(fnr(__y, __y_hat), 0.7272727272727273), \"False positive rate: ❌\"\n",
        "assert np.isclose(tnr(__y, __y_hat), 0.4), \"True negative rate: ❌\"\n",
        "assert np.isclose(fpr(__y, __y_hat), 0.6), \"False negative rate: ❌\"\n",
        "assert np.isclose(pr(__y, __y_hat), 0.3333333333333333), \"Precision: ❌\"\n",
        "assert np.isclose(rc(__y, __y_hat), 0.2727272727272727), \"Recall: ❌\"\n",
        "assert np.isclose(accuracy(__y, __y_hat), 0.3333333333333333), \"Accuracy: ❌\"\n",
        "assert np.isclose(error(__y, __y_hat), 0.6666666666666667), \"Error: ❌\"\n",
        "\n",
        "\n",
        "print(\"All metrics have passed! ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fSNxlmjajGr"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters for SAE\n",
        "params = SAEParams(0.5, 0.01, 5, 16)\n",
        "\n",
        "sae = train(toy_x, toy_y, params, 0)\n",
        "\n",
        "# Forward pass to get predictions\n",
        "_, Y_hat = forward(sae, toy_x)\n",
        "\n",
        "\n",
        "# Convert probabilities to one-hot encoding by maximizing predictions\n",
        "Y_hat = oneHotMax(Y_hat)\n",
        "\n",
        "\n",
        "# Initialize arrays to hold the metrics for each class (10 classes)\n",
        "col_tp = np.zeros(10, dtype=int)\n",
        "col_tn = np.zeros(10, dtype=int)\n",
        "col_fp = np.zeros(10, dtype=int)\n",
        "col_fn = np.zeros(10, dtype=int)\n",
        "col_tpr = np.zeros(10)\n",
        "col_fnr = np.zeros(10)\n",
        "col_tnr = np.zeros(10)\n",
        "col_fpr = np.zeros(10)\n",
        "col_pr = np.zeros(10)\n",
        "col_rc = np.zeros(10)\n",
        "col_acc = np.zeros(10)\n",
        "col_err = np.zeros(10)\n",
        "\n",
        "# Labels for each digit\n",
        "labels = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
        "\n",
        "# Loop through each class (0 to 9) and calculate metrics\n",
        "for class_idx in range(10):\n",
        "    col_tp[class_idx] = tp(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_tn[class_idx] = tn(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_fp[class_idx] = fp(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_fn[class_idx] = fn(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_tpr[class_idx] = tpr(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_fnr[class_idx] = fnr(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_tnr[class_idx] = tnr(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_fpr[class_idx] = fpr(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_pr[class_idx] = pr(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_rc[class_idx] = rc(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_acc[class_idx] = accuracy(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "    col_err[class_idx] = error(toy_y[:, class_idx], Y_hat[:, class_idx])\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "df = pd.DataFrame({\n",
        "    \"Digit\": labels,\n",
        "    \"True Positive\": col_tp,\n",
        "    \"True Negative\": col_tn,\n",
        "    \"False Positive\": col_fp,\n",
        "    \"False Negative\": col_fn,\n",
        "    \"True Positive Rate\": col_tpr,\n",
        "    \"False Negative Rate\": col_fnr,\n",
        "    \"True Negative Rate\": col_tnr,\n",
        "    \"False Positive Rate\": col_fpr,\n",
        "    \"Precision\": col_pr,\n",
        "    \"Recall\": col_rc,\n",
        "    \"Accuracy\": col_acc,\n",
        "    \"Error\": col_err\n",
        "})\n",
        "\n",
        "# Set display options for better tabular visualization with scrolling\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.width', None)        # No wrapping of columns\n",
        "pd.set_option('display.max_rows', None)     # Show all rows\n",
        "pd.set_option('display.max_colwidth', None) # Show all the content without truncating\n",
        "\n",
        "# Display the DataFrame as a proper table\n",
        "display(df)  # For Jupyter notebook or IPython\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU7W57XtJ24z"
      },
      "source": [
        "\n",
        "# **Selecting Hyperparameters**\n",
        "\n",
        "The supervised autoencoder class has 4 hyperparameters that need to be selected (for our implementation).\n",
        "So far we have worked with toy data and a few hardcoded hyperparameters that we know work okay, but now it's time to start tuning our algorithm to get the best performance.\n",
        "\n",
        "## **Internal K-Fold Cross-Validation**\n",
        "In this section, you will implement **internal k-fold cross-validation** to select hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u82ky17KdCls"
      },
      "outputs": [],
      "source": [
        "def generate_param_settings(βs, αs, epochss, hiddens):\n",
        "    # Create the Cartesian product of all parameters\n",
        "    # This produces an iterator over tuples (β, α, epochs, hidden)\n",
        "    param_combinations = product(βs, αs, epochss, hiddens)\n",
        "\n",
        "    # For each tuple, create an SAEParams instance by passing the tuple to the constructor using the vals parameter\n",
        "    return [SAEParams(vals=vals) for vals in param_combinations]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gByeAnqQKy-r"
      },
      "outputs": [],
      "source": [
        "def stack(arr):\n",
        "    \"\"\"\n",
        "    Stacks matrices vertically.\n",
        "    For example, if given matrices with shapes:\n",
        "      A = (10, 23), B = (15, 23), C = (2, 23)\n",
        "    then stack([A, B, C]) returns a (27, 23) matrix.\n",
        "    \"\"\"\n",
        "    return np.vstack(arr)\n",
        "\n",
        "def exclude(arr, idx):\n",
        "    \"\"\"\n",
        "    Returns all elements of a list (or 1D array) except the element at the specified index.\n",
        "    For example, if arr = [a, b, c, d] and idx = 2, this returns [a, b, d].\n",
        "    \"\"\"\n",
        "    return [element for i, element in enumerate(arr) if i != idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnj1FgSlLnii"
      },
      "source": [
        "\n",
        "We can choose to optimize any of the above metrics that we would like. By default, we have selected $\\verb+accuracy+$. You can try other metrics, if you would like.\n",
        "Below, we use `argmax` when selecting hyperparameters, because we want high accuracy (if we had errors, we would use `argmin`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPC1ye3QLkcl"
      },
      "outputs": [],
      "source": [
        "def metric(sae, X, Y, f=accuracy):\n",
        "    \"\"\"\n",
        "    Compute the average metric (e.g. accuracy) over each class.\n",
        "\n",
        "    Parameters:\n",
        "      sae : the model\n",
        "      X   : input data as a 2D array (samples x features)\n",
        "      Y   : true labels as a 2D array (samples x classes)\n",
        "      f   : a function that computes a metric for two 1D arrays (default: accuracy)\n",
        "\n",
        "    Returns:\n",
        "      The mean of the metric computed for each class.\n",
        "    \"\"\"\n",
        "    # Run forward propagation on the model\n",
        "    # Assume forward(sae, X) returns (unused, Yhat)\n",
        "    _, Yhat = forward(sae, X)\n",
        "\n",
        "    # Apply one_hot_max to Yhat to get one-hot encoded predictions\n",
        "    Yhat = oneHotMax(Yhat)\n",
        "\n",
        "    # Determine the number of classes from Y's shape\n",
        "    classes = Y.shape[1]\n",
        "\n",
        "    # Initialize a metric value for each class\n",
        "    metrics = np.zeros(classes)\n",
        "\n",
        "    # Compute the metric for each class by comparing the true and predicted values column-wise\n",
        "    for class_idx in range(classes):\n",
        "        metrics[class_idx] = f(Y[:, class_idx], Yhat[:, class_idx])\n",
        "\n",
        "    # Return the average metric across classes\n",
        "    return np.mean(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o35xvx78MKbL"
      },
      "source": [
        "\n",
        "Now you will implement `internal_cross_validation`.\n",
        "Make sure that each of your `k` folds are non-overlapping and that every validation set is used exactly once.\n",
        "We explicitly fix the random seed when training the model.\n",
        "We want to reduce the number of confounding factors while selecting hyperparameters, for instance starting each neural network with the exact same set of weights. That way we are removing one potential source of stochasticity that could give noisier estimates of the performance for a hyperparameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE_INDoaMKO7"
      },
      "outputs": [],
      "source": [
        "def internal_cross_validation(settings, k, X, Y, comparator=accuracy):\n",
        "    \"\"\"\n",
        "    Performs internal cross validation over a list of hyperparameter settings.\n",
        "\n",
        "    Parameters:\n",
        "      settings   : list of SAEParams (or equivalent parameter settings)\n",
        "      k          : number of folds to use for cross validation\n",
        "      X          : input data (NumPy array of shape (samples, features))\n",
        "      Y          : true labels (NumPy array of shape (samples, classes))\n",
        "      comparator : function to compute a metric (default is accuracy)\n",
        "\n",
        "    Returns:\n",
        "      best_params: the hyperparameter setting (from settings) that achieves the best average metric\n",
        "    \"\"\"\n",
        "    # Partition the data into k folds without shuffling\n",
        "    Xs = []\n",
        "    Ys = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    # Use a fixed seed so every hyperparameter setting sees the same random seed\n",
        "    seed = 0\n",
        "\n",
        "    average_metrics = np.zeros(len(settings))\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # Evaluate each parameter setting using k-fold cross validation\n",
        "    for idx, params in enumerate(settings):\n",
        "\n",
        "\n",
        "\n",
        "        for fold in range(k):\n",
        "            # Create training data by stacking all folds except the current one\n",
        "\n",
        "\n",
        "\n",
        "            # The validation data is the current fold\n",
        "\n",
        "\n",
        "\n",
        "            # Train the model with the given parameter settings and seed\n",
        "\n",
        "\n",
        "            # Compute the metric on the validation set\n",
        "\n",
        "\n",
        "        # Store the average metric for the current parameter setting\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "    best_idx = np.argmax(average_metrics)\n",
        "    best_params = settings[best_idx]\n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeW2MuOUNQhK"
      },
      "outputs": [],
      "source": [
        "# #####################\n",
        "# Test Block\n",
        "# #####################\n",
        "\n",
        "# Test block for internal cross validation\n",
        "\n",
        "np.random.seed(367)\n",
        "\n",
        "# Generate the hyperparameter settings.\n",
        "settings = generate_param_settings([100, 1., 0.1],\n",
        "                                    [0.001, 0.1],\n",
        "                                    [1, 3, 5, 7],\n",
        "                                    [2, 4, 6, 8])\n",
        "\n",
        "# Generate synthetic data:\n",
        "# __x is a 100 x 3 matrix drawn from Normal(0, 3)\n",
        "__x = np.random.normal(0, 3, size=(100, 3))\n",
        "# __y is a 100 x 2 matrix where each entry is drawn from a Bernoulli(0.5)\n",
        "__y = np.random.binomial(1, 0.5, size=(100, 2)).astype(np.float32)\n",
        "\n",
        "# Perform internal cross validation using 3 folds.\n",
        "got = internal_cross_validation(settings, 3, __x, __y, comparator=accuracy)\n",
        "\n",
        "# The expected best parameter\n",
        "expected = SAEParams(0.1, 0.1 , 3, 4)\n",
        "\n",
        "# Check if the returned parameter settings match the expected one.\n",
        "# (Assuming SAEParams does not implement __eq__, we compare fields manually.)\n",
        "is_equal = (got.β == expected.β and\n",
        "            got.alpha == expected.alpha and\n",
        "            got.epochs == expected.epochs and\n",
        "            got.hidden == expected.hidden)\n",
        "\n",
        "\n",
        "assert got.β == expected.β, \"Wrong value for β: ❌\"\n",
        "assert got.alpha == expected.alpha, \"Wrong value for alpha: ❌\"\n",
        "assert got.epochs == expected.epochs, \"Wrong value for epochs: ❌\"\n",
        "assert got.hidden == expected.hidden, \"Wrong value for hidden: ❌\"\n",
        "\n",
        "\n",
        "print(\"Internal cross validation test passed: ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTcJ_uUuPjEh"
      },
      "source": [
        "If all went well with your cross-validation code, then it will pass our tests and you should see a happy check mark above.\n",
        "\n",
        "Now, let's run internal CV on our small MNIST dataset, to see which hyperparameter settings are picked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSrOTMIJOv9n"
      },
      "outputs": [],
      "source": [
        "# Note: This cell may take several seconds to complete (e.g., ~30 seconds)\n",
        "\n",
        "# Generate the hyperparameter settings.\n",
        "settings = generate_param_settings(\n",
        "    [1.0, 0.1],    # betas\n",
        "    [0.01, 0.001], # alphas\n",
        "    [2],           # epochs\n",
        "    [16, 32]       # hidden units\n",
        ")\n",
        "\n",
        "# Of these parameter settings, let's see which one is the best on our small dataset.\n",
        "# Note: This is only for a few hundred samples. Imagine running this for bigger models and datasets.\n",
        "# Feel free to change the `comparator` parameter to try other metrics.\n",
        "best_toy_param = internal_cross_validation(settings, 4, toy_x, toy_y, comparator=accuracy)\n",
        "\n",
        "# Optionally, you can print the result to inspect the best parameter setting.\n",
        "print(\"Best toy parameter setting:\")\n",
        "print(\"Best β\", best_toy_param.β)\n",
        "print(\"Best alpha\", best_toy_param.alpha)\n",
        "print(\"Best number epochs\", best_toy_param.epochs)\n",
        "print(\"Best number of hidden units\", best_toy_param.hidden)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJtE9aeQ3Q7"
      },
      "source": [
        "# **Evaluating Generalization Error**\n",
        "\n",
        "Internal CV was used above to select hyperparameters $\\verb+best-hypers+$ for the SAE. The model f learned with SAE+best-hypers on the entire dataset is the final model that we want to deploy. However, before deploying, we want to get a sense of its generalization performance (classification accuracy in deployment). In the notes we discussed using external CV, where we would split up the dataset into train/validate pairs like above, and then call SAE+internal CV on these train/validate pairs. This is called nested cross-validation: we have nested for loops where in the outer loop we iterate over the splits from external CV, and inside when we call SAE+internal CV, it splits the data further into multiple folds and loops over all hypers and those partitionings.\n",
        "\n",
        "Unfortunately, this is very expensive. And already running on this MNIST dataset can take a bit too much compute for a student laptop. So, we are going to do a standard approximation used in practice, where we do a two-stage approach (see the notes for why this is more biased and can cause an overly optimistic estimate). Instead of using nest cross-validation to properly evaluate f, we will instead just call cross-validation on SAE+best-hypers. To implement cross-validation to evaluate SAE+best-hypers, we will use repeated random sampling (RRS). The main reason we use RRS instead of k-fold for our external CV is because it actually helps reduce bias a little bit from this un-nested procedure, and because it lets you implement the other way approach we discussed to generated partitions for CV. We will report the average and standard error of our accuracy across these partitions.\n",
        "\n",
        "A few notes:\n",
        " * The train/test sets must be non-overlapping\n",
        " * Each train/test split needs to be sampled randomly `k` times\n",
        " * You should use `train_percent` percentage of the samples in the train set, and the remainder in the test set (i.e. if `train_percent=0.9`, then 90% of the samples are training samples and 10% are testing samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNyjOw-uQuXD"
      },
      "outputs": [],
      "source": [
        "def repeated_random_sampling(params, X, Y, k, train_percent, rng, comparator=accuracy):\n",
        "    \"\"\"\n",
        "    Performs repeated random sampling cross validation.\n",
        "\n",
        "    Parameters:\n",
        "      params        : SAEParams instance containing hyperparameters.\n",
        "      X             : NumPy array of shape (samples, features).\n",
        "      Y             : NumPy array of shape (samples, classes).\n",
        "      k             : Number of repetitions.\n",
        "      train_percent : Fraction (float) of data to use for training.\n",
        "      rng           : A NumPy random generator (e.g., np.random.default_rng(seed)).\n",
        "      comparator    : A metric function (default: accuracy).\n",
        "\n",
        "    Returns:\n",
        "      A tuple (mean_metric, std_error) where:\n",
        "        - mean_metric is the average metric across the k folds,\n",
        "        - std_error is the standard error (std / sqrt(k)).\n",
        "    \"\"\"\n",
        "    samples = X.shape[0]\n",
        "    train_samples = int(np.floor(samples * train_percent))\n",
        "\n",
        "    # Ensure consistent initialization across folds.\n",
        "    seed = 0\n",
        "\n",
        "    metrics = np.zeros(k)\n",
        "\n",
        "\n",
        "    for fold in range(k):\n",
        "        print(\"rrs\", fold + 1, k)\n",
        "\n",
        "        idxs = rng.permutation(samples)\n",
        "\n",
        "        ### BEGIN SOLUTION\n",
        "\n",
        "        # Rearrange X and Y according to the shuffled indices.\n",
        "\n",
        "\n",
        "\n",
        "        # Split into training and test sets.\n",
        "\n",
        "\n",
        "\n",
        "        # Train the model using the given parameters and fixed seed.\n",
        "\n",
        "\n",
        "        # Compute the metric on the test set.\n",
        "\n",
        "\n",
        "        ### END SOLUTION\n",
        "\n",
        "    # Return the mean metric and the standard error.\n",
        "    return np.mean(metrics), np.std(metrics) / np.sqrt(k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNg0FIqhHawV"
      },
      "outputs": [],
      "source": [
        "# Create the parameter setting\n",
        "params = SAEParams(0.1, 0.001, 2, 10)\n",
        "\n",
        "# Create a random generator with seed 1\n",
        "rng = np.random.default_rng(1)\n",
        "\n",
        "# Generate synthetic data:\n",
        "# X: 500 x 30 matrix drawn from a Normal distribution with mean=1 and std=20\n",
        "X = rng.normal(loc=1, scale=10, size=(500, 30))\n",
        "# Y: 500 x 3 matrix with entries drawn from a Bernoulli(0.4) distribution\n",
        "Y = rng.binomial(n=1, p=0.4, size=(500, 3)).astype(np.float32)\n",
        "\n",
        "# Run repeated random sampling cross validation with 9 repetitions,\n",
        "# using 42% of the samples for training.\n",
        "acc, stderr = repeated_random_sampling(params, X, Y, 9, 0.42, rng, comparator=accuracy)\n",
        "\n",
        "print(acc, stderr)\n",
        "# Return True if the accuracy is at least 0.5 and the standard error is <= 0.01.\n",
        "assert acc >= 0.5 and stderr <= 0.01, \"Repeated random sampling test failed: ❌\"\n",
        "\n",
        "print(\"Repeated random sampling test passed:.✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGDIegSVYpMx"
      },
      "source": [
        "Does RRS report the correct accuracy and standard error on a small toy dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOch6PvpH_l0"
      },
      "outputs": [],
      "source": [
        "# Create a random generator with seed 3\n",
        "rng = np.random.default_rng(3)\n",
        "\n",
        "# Call repeated_random_sampling with 10 repetitions and a training fraction of 0.90.\n",
        "result = repeated_random_sampling(best_toy_param, toy_x, toy_y, 10, 0.90, rng)\n",
        "\n",
        "print(\"Accuracy:\", result[0], \"Std Error:\", result[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUTy_IKAZOFk"
      },
      "source": [
        "# Putting it all together\n",
        "\n",
        "Finally, we will run the SAE with internal CV to get best-hypers on the full MNIST dataset and then run external CV to evaluate the model learned with those hypers. If you want to examine other metrics, you can change the metric used in RRS and the outputted numbers will be the mean and standard error across the folds for that metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHJw6sZ2ZKlw"
      },
      "outputs": [],
      "source": [
        "def run_experiment():\n",
        "    # Load 10,000 MNIST samples\n",
        "    X, Y = prepMNIST(5000)\n",
        "\n",
        "    # Generate hyperparameter settings:\n",
        "    # beta: [0.1, 0.0], alpha: [0.001, 0.01], epochs: [1, 2], hidden units: [50, 100]\n",
        "    settings = generate_param_settings(\n",
        "        [0.1, 0.0],     # beta\n",
        "        [0.001, 0.01],  # alpha\n",
        "        [1, 2],         # epochs\n",
        "        [50, 100]       # hidden units\n",
        "    )\n",
        "\n",
        "    # Perform internal cross validation with 3 folds to select the best parameters.\n",
        "    best_params = internal_cross_validation(settings, 3, X, Y)\n",
        "    print(\"Best β: \", best_params.β)\n",
        "    print(\"Best alpha: \", best_params.alpha)\n",
        "    print(\"Best number of epochs: \", best_params.epochs)\n",
        "    print(\"Best hidden units size: \", best_params.hidden)\n",
        "\n",
        "    # Create a random generator with seed 734\n",
        "    rng = np.random.default_rng(734)\n",
        "\n",
        "    # Run repeated random sampling using the best parameters, 5 repetitions, and a training fraction of 0.75.\n",
        "    acc, stderr = repeated_random_sampling(best_params, X, Y, 5, 0.75, rng)\n",
        "\n",
        "    return acc, stderr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sv_pnIAZwhR"
      },
      "outputs": [],
      "source": [
        "## uncomment the following when you are ready to run it\n",
        "## it will approximately 10~15m on colab\n",
        "\n",
        "accuracy_val, std_err = run_experiment()\n",
        "print(\"Repeated Random Sampling -> Accuracy:\", accuracy_val, \"Std Error:\", std_err)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}